{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "A_CaQtFFrYTQ",
        "outputId": "67a444d5-0e45-455d-b2b3-412e723c92a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b4f6184e-2cb7-41b0-859e-b9bc8535434b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b4f6184e-2cb7-41b0-859e-b9bc8535434b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving js_model.pt to js_model.pt\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Upload and Load Data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk-OXUyGrRh2",
        "outputId": "79149933-9553-4a9d-d3b5-099168e908f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 123889 code snippets.\n",
            "function createTypeScriptLanguageService(options) {\n",
            "    // Discover referenced files\n",
            "    const FILES = discoverAndReadFiles(options);\n",
            "    // Add fake usage files\n",
            "    options.inlineEntryPoints.forEach((inlineEntryPoint, index) => {\n",
            "        FILES[`inlineEntryPoint.${index}.ts`] = inlineEntryPoint;\n",
            "   \n",
            "step 0: train loss 10.9358, val loss 10.9452\n",
            "step 200: train loss 3.1363, val loss 3.2670\n",
            "step 400: train loss 2.8535, val loss 3.0104\n",
            "step 600: train loss 2.6741, val loss 2.8578\n",
            "step 800: train loss 2.5689, val loss 2.6966\n",
            "step 999: train loss 2.4646, val loss 2.6309\n",
            "!githubSetGrid', 'option'])\n",
            "         glyphDistafWatch': this.draw1\n",
            "         wrapper.menu = this;\n",
            "       }function |version(value) {\n",
            "        this.map.charAt(0,{\"0)|0)/ phase,\n",
            "          = [];\n",
            "    }\n",
            "      this.ok(enendServices.DD, \"\";\n",
            "       gl.dirtyGW++;\n",
            "      }\n",
            "    }\n",
            "\n",
            "lim\n",
            "  function ensureAsegment.addLine 24Lum(event)AM 398\n",
            "     ; = me.each(0, data.body);\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "data = []\n",
        "for i in range(5):\n",
        "    with gzip.open(f\"javascript_train_{i}.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "          obj = json.loads(line)\n",
        "          code = obj.get(\"code\", \"\").strip()\n",
        "          if code:\n",
        "              data.append(code)\n",
        "\n",
        "print(f\"Loaded {len(data)} code snippets.\")\n",
        "print(data[0][:300])  # preview\n",
        "\n",
        "# Step 2: Tokenize using tiktoken\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "tokens = []\n",
        "for code in data:\n",
        "    tokens.extend(enc.encode(code))\n",
        "\n",
        "import torch\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "train_data = tokens[:int(0.9 * len(tokens))]\n",
        "val_data = tokens[int(0.9 * len(tokens)):]\n",
        "\n",
        "# Step 3: Batching logic\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# Step 4: Transformer model definition\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, self.n_head, 3 * self.head_size).transpose(1, 2)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, d_ff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ff = FeedForward(n_embd, d_ff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, d_ff, n_layer, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(d_model, n_head, d_ff) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Step 5: Model Initialization\n",
        "vocab_size = enc.n_vocab\n",
        "model = Transformer(vocab_size, d_model=256, n_head=4, d_ff=1024, n_layer=4, block_size=block_size).to(device)\n",
        "\n",
        "# Step 6: Training\n",
        "max_iters = 1000\n",
        "eval_interval = 200\n",
        "eval_iters = 20\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Step 7: Generate Code\n",
        "context = torch.zeros((1, 1), dtype=torch.long).to(device)  # BOS token or start of context\n",
        "output = model.generate(context, max_new_tokens=200)\n",
        "print(enc.decode(output[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNQRTSez87S"
      },
      "outputs": [],
      "source": [
        "# Damn thats worst output :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SA1WgzVsLrQ",
        "outputId": "e6874fe8-f9e8-499e-81af-bc9b593c5f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 2.4721, val loss 2.6887\n",
            "step 500: train loss 2.2813, val loss 2.5202\n",
            "step 1000: train loss 2.1846, val loss 2.4043\n",
            "step 1500: train loss 2.0603, val loss 2.2678\n",
            "step 2000: train loss 2.0285, val loss 2.3163\n",
            "step 2500: train loss 1.9920, val loss 2.2856\n",
            "step 3000: train loss 1.9648, val loss 2.2177\n",
            "step 3500: train loss 1.8996, val loss 2.1656\n",
            "step 4000: train loss 1.8547, val loss 2.1236\n",
            "step 4500: train loss 1.8032, val loss 2.1330\n",
            "step 4999: train loss 1.8137, val loss 2.0508\n"
          ]
        }
      ],
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 20\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjyKZZk9wgYl",
        "outputId": "4934fb84-6965-4e2a-cce1-2885c8c31059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!picker(d), ranges / 180Filters.addClass(elementsBy.CLASSins.disabled)*values.midSegmentCore.kill.isaSlug),\n",
            "\t\t\t\t\ttime = allDialog.datatableRegions.gasizeDetectorHape({}, settingsByTODES.api.position)\n",
            "\t\t\t\t\t}).join(\",groupName).map(V => {\n",
            "           var self = this[esByHOUREqual(iPodFnPropertyToken),\n",
            "\t             values = columns.subnumericIndexOfSlrQRTLim(iVBatorsCommaFirstTokenEquals, name));\n",
            "           var possibleId = models[i];\n",
            "\n",
            "          if (\n",
            "       \n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long).to(device)  # BOS token or start of context\n",
        "output = model.generate(context, max_new_tokens=200)\n",
        "print(enc.decode(output[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_FnYXEGz0N7"
      },
      "outputs": [],
      "source": [
        "# magic <_<\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v0SVMWb1OyX",
        "outputId": "800dc96c-206f-4da1-b8fb-b1ac96e08699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 1.7834, val loss 2.0846\n",
            "step 500: train loss 1.7990, val loss 1.9742\n",
            "step 1000: train loss 1.7302, val loss 2.0430\n",
            "step 1500: train loss 1.8141, val loss 1.9653\n",
            "step 2000: train loss 1.7216, val loss 1.9607\n",
            "step 2500: train loss 1.7387, val loss 1.9547\n",
            "step 3000: train loss 1.7211, val loss 1.9730\n"
          ]
        }
      ],
      "source": [
        "max_iters = 20000\n",
        "eval_interval = 500\n",
        "eval_iters = 20\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IumquQwi1nGB"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"js_model.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "VOZT7-pg7fMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "viEmQBJfmC9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "data = []\n",
        "for i in range(5):\n",
        "    with gzip.open(f\"javascript_train_{i}.jsonl.gz\", \"rt\", encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "          obj = json.loads(line)\n",
        "          code = obj.get(\"code\", \"\").strip()\n",
        "          if code:\n",
        "              data.append(code)\n",
        "\n",
        "print(f\"Loaded {len(data)} code snippets.\")\n",
        "print(data[0][:300])  # preview\n",
        "\n",
        "# Step 2: Tokenize using tiktoken\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "tokens = []\n",
        "for code in data:\n",
        "    tokens.extend(enc.encode(code))\n",
        "\n",
        "import torch\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "train_data = tokens[:int(0.9 * len(tokens))]\n",
        "val_data = tokens[int(0.9 * len(tokens)):]\n",
        "\n",
        "# Step 3: Batching logic\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWCQWSaGNDxE",
        "outputId": "ad4dad62-e9a7-4e62-968e-460f9bbe97d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 123889 code snippets.\n",
            "function createTypeScriptLanguageService(options) {\n",
            "    // Discover referenced files\n",
            "    const FILES = discoverAndReadFiles(options);\n",
            "    // Add fake usage files\n",
            "    options.inlineEntryPoints.forEach((inlineEntryPoint, index) => {\n",
            "        FILES[`inlineEntryPoint.${index}.ts`] = inlineEntryPoint;\n",
            "   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Transformer model definition\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, self.n_head, 3 * self.head_size).transpose(1, 2)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, d_ff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ff = FeedForward(n_embd, d_ff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, d_ff, n_layer, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(d_model, n_head, d_ff) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "DkmiLg2hMeQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = enc.n_vocab\n",
        "model = Transformer(vocab_size, d_model=256, n_head=4, d_ff=1024, n_layer=4, block_size=128).to(device)\n",
        "model.load_state_dict(torch.load(\"js_model.pt\", map_location=device))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9QewVTYLbRR",
        "outputId": "d0b6fb84-83e0-492c-e8be-c5224ec53af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (token_emb): Embedding(50257, 256)\n",
              "  (pos_enc): PositionalEncoding()\n",
              "  (blocks): ModuleList(\n",
              "    (0-3): 4 x Block(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=256, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Half-written JavaScript code prompt\n",
        "prompt = \"\"\"\n",
        "function sum(a, b) {\n",
        "  return\n",
        "\"\"\"\n",
        "\n",
        "# Encode the prompt using your tokenizer\n",
        "input_ids = torch.tensor(enc.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)  # shape: [1, T]\n",
        "\n",
        "# Generate continuation\n",
        "output = model.generate(input_ids, max_new_tokens=100)\n",
        "\n",
        "# Decode and print result\n",
        "print(enc.decode(output[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya0rTDq5Li_0",
        "outputId": "f63f0784-6f67-4e14-8a22-b0015e9fc331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "function sum(a, b) {\n",
            "  return\n",
            "    / beautify #\\s*    kBottom of format( |1974):\\/\\/-[negative Lookup>\"\n",
            "             +\n",
            "                   | \" +\n",
            "                    tp(a, rNode, y),\n",
            "      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Input/output\n",
        "output_path = \"clean_code.jsonl\"\n",
        "\n",
        "def is_clean_code(code):\n",
        "    if len(code) < 50 or len(code) > 5000:\n",
        "        return False\n",
        "    if code.count('\\n') < 2:\n",
        "        return False\n",
        "    if \"eval(\" in code or \"Function(\" in code or \"document.write\" in code:\n",
        "        return False\n",
        "    if re.search(r\"\\b[a-zA-Z]{1,2}\\b\", code):\n",
        "        short_vars = re.findall(r\"\\b[a-zA-Z]{1,2}\\b\", code)\n",
        "        if len(short_vars) > 15:\n",
        "            return False\n",
        "    if re.search(r\"[\\u0000-\\u001f]\", code):  # control characters\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def normalize_code(code):\n",
        "    return \"\\n\".join(\n",
        "        line.strip().replace('\\t', '   ')\n",
        "        for line in code.strip().splitlines()\n",
        "        if line.strip()\n",
        "    )\n",
        "\n",
        "# Clean across all files\n",
        "clean_count = 0\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    for i in range(5):\n",
        "        fname = f\"javascript_train_{i}.jsonl.gz\"\n",
        "        try:\n",
        "            with gzip.open(fname, \"rt\", encoding=\"utf-8\") as f_in:\n",
        "                for line in f_in:\n",
        "                    obj = json.loads(line)\n",
        "                    code = obj.get(\"code\", \"\").strip()\n",
        "                    if is_clean_code(code):\n",
        "                        clean_code = normalize_code(code)\n",
        "                        json.dump({\"code\": clean_code}, f_out)\n",
        "                        f_out.write(\"\\n\")\n",
        "                        clean_count += 1\n",
        "        except FileNotFoundError:\n",
        "            print(f\"⚠️  File not found: {fname}, skipping.\")\n",
        "\n",
        "print(f\"✅ Done. Saved {clean_count} clean code snippets to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8ThAHMxOlH-",
        "outputId": "c32ac754-bcaf-415c-80fd-4c1c037aa7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done. Saved 0 clean code snippets to clean_code.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"clean_code.jsonl\", \"r\") as f:\n",
        "    data = [json.loads(line)[\"code\"] for line in f]\n"
      ],
      "metadata": {
        "id": "2nYTavQiMRM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzFSrELxMZum",
        "outputId": "06674998-68b5-4feb-9f83-03c8563d70e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Output file\n",
        "output_path = \"clean_code.jsonl\"\n",
        "\n",
        "# Heuristics for good code\n",
        "def is_clean_code(code, docstring):\n",
        "    if not code or len(code) < 30 or len(code) > 3000:\n",
        "        return False\n",
        "    if \"eval(\" in code or \"Function(\" in code or \"document.write\" in code:\n",
        "        return False\n",
        "    if code.count('\\n') < 1:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# Normalization (light cleanup)\n",
        "def normalize_code(code):\n",
        "    return \"\\n\".join(\n",
        "        line.strip().replace('\\t', '    ')\n",
        "        for line in code.strip().splitlines()\n",
        "        if line.strip()\n",
        "    )\n",
        "\n",
        "clean_count = 0\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    for i in range(5):\n",
        "        fname = f\"javascript_train_{i}.jsonl.gz\"\n",
        "        try:\n",
        "            with gzip.open(fname, \"rt\", encoding=\"utf-8\") as f_in:\n",
        "                for line in f_in:\n",
        "                    try:\n",
        "                        obj = json.loads(line)\n",
        "                        code = obj.get(\"code\", \"\").strip()\n",
        "                        docstring = obj.get(\"docstring\", \"\").strip()\n",
        "                        if is_clean_code(code, docstring):\n",
        "                            clean_code = normalize_code(code)\n",
        "                            json.dump({\"code\": clean_code, \"docstring\": docstring}, f_out)\n",
        "                            f_out.write(\"\\n\")\n",
        "                            clean_count += 1\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "print(f\"✅ Done. Saved {clean_count} clean code+docstring pairs to {output_path}\")\n"
      ],
      "metadata": {
        "id": "NnJZumt0MfQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10aa7eee-c088-4e05-d7cf-5702f9bf0883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done. Saved 116120 clean code+docstring pairs to clean_code.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"clean_code.jsonl\", \"r\") as f:\n",
        "    data = [json.loads(line)[\"code\"] for line in f]"
      ],
      "metadata": {
        "id": "3ZRwLtHjmlf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-cMAF6Om2KN",
        "outputId": "58ebf046-e872-4685-90e1-938558932746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['function createTypeScriptLanguageService ( options ) { / / Discover referenced files const FILES = discoverAndReadFiles ( options ) ; / / Add fake usage files options . inlineEntryPoints . forEach ( ( inlineEntryPoint , index ) = > { FILES [ ` inlineEntryPoint . $ { index } . ts ` ] = inlineEntryPoint ; } ) ; / / Add additional typings options . typings . forEach ( ( typing ) = > { const filePath = path . join ( options . sourcesRoot , typing ) ; FILES [ typing ] = fs . readFileSync ( filePath ) . toString ( ) ; } ) ; / / Resolve libs const RESOLVED _ LIBS = { } ; options . libs . forEach ( ( filename ) = > { const filepath = path . join ( TYPESCRIPT _ LIB _ FOLDER , filename ) ; RESOLVED _ LIBS [ ` defaultLib : $ { filename } ` ] = fs . readFileSync ( filepath ) . toString ( ) ; } ) ; const compilerOptions = ts . convertCompilerOptionsFromJson ( options . compilerOptions , options . sourcesRoot ) . options ; const host = new TypeScriptLanguageServiceHost ( RESOLVED _ LIBS , FILES , compilerOptions ) ; return ts . createLanguageService ( host ) ; }']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ========== Normalization Patterns ==========\n",
        "normalize1 = [\n",
        "    ('<skipped>', ''),\n",
        "    (r'-\\n', ''),\n",
        "    (r'\\n', ' ')\n",
        "]\n",
        "normalize1 = [(re.compile(pattern), repl) for pattern, repl in normalize1]\n",
        "\n",
        "normalize2 = [\n",
        "    (r'([\\{-\\~\\[-\\` -\\&\\(-\\+\\:-\\@\\/])', r' \\1 '),\n",
        "    (r'([^0-9])([\\.,])', r'\\1 \\2 '),\n",
        "    (r'([\\.,])([^0-9])', r' \\1 \\2'),\n",
        "    (r'([0-9])(-)', r'\\1 \\2 ')\n",
        "]\n",
        "normalize2 = [(re.compile(pattern), repl) for pattern, repl in normalize2]\n",
        "\n",
        "def normalize_code(code):\n",
        "    code = code.strip()\n",
        "    for pattern, repl in normalize1:\n",
        "        code = pattern.sub(repl, code)\n",
        "    for pattern, repl in normalize2:\n",
        "        code = pattern.sub(repl, code)\n",
        "    return ' '.join(code.split())  # remove redundant spaces\n",
        "\n",
        "# ========== Optional: Filtering Function ==========\n",
        "def is_clean_code(code):\n",
        "    if len(code) < 10: return False\n",
        "    if 'eval' in code or 'base64' in code: return False\n",
        "    if len(code.splitlines()) < 2: return False\n",
        "    if len(set(code)) < 10: return False\n",
        "    return True\n",
        "\n",
        "# ========== Output Path ==========\n",
        "output_path = \"clean_code.jsonl\"\n",
        "\n",
        "# ========== Clean & Save ==========\n",
        "clean_count = 0\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "    for i in range(5):\n",
        "        input_path = f\"javascript_train_{i}.jsonl.gz\"\n",
        "        if not os.path.exists(input_path):\n",
        "            print(f\"⚠️ File not found: {input_path}\")\n",
        "            continue\n",
        "        with gzip.open(input_path, \"rt\", encoding=\"utf-8\") as f_in:\n",
        "            for line in f_in:\n",
        "                try:\n",
        "                    obj = json.loads(line)\n",
        "                    code = obj.get(\"code\", \"\").strip()\n",
        "                    if is_clean_code(code):\n",
        "                        clean_code = normalize_code(code)\n",
        "                        json.dump({\"code\": clean_code}, f_out)\n",
        "                        f_out.write(\"\\n\")\n",
        "                        clean_count += 1\n",
        "                except Exception as e:\n",
        "                    continue  # skip bad lines\n",
        "\n",
        "print(f\"✅ Done. Saved {clean_count} clean code snippets to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIr5uS28m3C_",
        "outputId": "238b16e0-c4a0-4d51-81f2-f891f5091092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done. Saved 122113 clean code snippets to clean_code.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "OifajUNr6nw6",
        "outputId": "8f50e967-af9d-4820-8fa6-9049ac4ef7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-4265195184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeImplicitAutograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2744\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorLikeType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_kernels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    128\u001b[0m                     \u001b[0;34mf\"Trying to override a python impl for {k} on operator {self.name()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "# Load tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Load cleaned JSONL file\n",
        "data = []\n",
        "with open(\"clean_code.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        obj = json.loads(line)\n",
        "        code = obj.get(\"code\", \"\")\n",
        "        if code:\n",
        "            data.append(code)\n",
        "\n",
        "print(f\"✅ Loaded {len(data)} cleaned code snippets.\")\n",
        "\n",
        "# Tokenize all code snippets into a flat list of token IDs\n",
        "all_tokens = []\n",
        "for snippet in data:\n",
        "    all_tokens.extend(enc.encode(snippet))\n",
        "\n",
        "print(f\"✅ Total tokens: {len(all_tokens)}\")\n",
        "\n",
        "# Convert tokens to a torch tensor\n",
        "tokens_tensor = torch.tensor(all_tokens, dtype=torch.long)\n",
        "\n",
        "# Train/val split (90/10)\n",
        "split_idx = int(0.9 * len(tokens_tensor))\n",
        "train_data = tokens_tensor[:split_idx]\n",
        "val_data = tokens_tensor[split_idx:]\n",
        "\n",
        "print(f\"Train shape: {train_data.shape}, Val shape: {val_data.shape}\")\n",
        "\n",
        "# ========== Batch Sampling Function ==========\n",
        "block_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# Quick test\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(\"Sample input decoded:\", enc.decode(xb[0].tolist()))\n",
        "print(\"Sample target decoded:\", enc.decode(yb[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5FjQpycgpq-d",
        "outputId": "c9d5910b-f14c-4194-b6f1-de571e2a3d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-1905439965.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeImplicitAutograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2743\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDispatchKey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2744\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensorLikeType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_kernels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    128\u001b[0m                     \u001b[0;34mf\"Trying to override a python impl for {k} on operator {self.name()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to override a python impl for DispatchKey.Meta on operator aten::broadcast_tensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8etdafLMqIJQ",
        "outputId": "96b99e5b-b950-4465-de1b-30b393fee9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['function createTypeScriptLanguageService ( options ) { / / Discover referenced files const FILES = discoverAndReadFiles ( options ) ; / / Add fake usage files options . inlineEntryPoints . forEach ( ( inlineEntryPoint , index ) = > { FILES [ ` inlineEntryPoint . $ { index } . ts ` ] = inlineEntryPoint ; } ) ; / / Add additional typings options . typings . forEach ( ( typing ) = > { const filePath = path . join ( options . sourcesRoot , typing ) ; FILES [ typing ] = fs . readFileSync ( filePath ) . toString ( ) ; } ) ; / / Resolve libs const RESOLVED _ LIBS = { } ; options . libs . forEach ( ( filename ) = > { const filepath = path . join ( TYPESCRIPT _ LIB _ FOLDER , filename ) ; RESOLVED _ LIBS [ ` defaultLib : $ { filename } ` ] = fs . readFileSync ( filepath ) . toString ( ) ; } ) ; const compilerOptions = ts . convertCompilerOptionsFromJson ( options . compilerOptions , options . sourcesRoot ) . options ; const host = new TypeScriptLanguageServiceHost ( RESOLVED _ LIBS , FILES , compilerOptions ) ; return ts . createLanguageService ( host ) ; }']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Transformer model definition\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.head_size = n_embd // n_head\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, self.n_head, 3 * self.head_size).transpose(1, 2)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * (self.head_size ** -0.5)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        out = attn @ v\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd, d_ff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ff = FeedForward(n_embd, d_ff)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_head, d_ff, n_layer, block_size):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(d_model, n_head, d_ff) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "elx87ymiqKVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3Cii3ywqgca"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}